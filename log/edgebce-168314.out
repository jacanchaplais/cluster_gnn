Running SLURM prolog script on indigo58.cluster.local
===============================================================================
Job started on Sun  9 May 11:23:21 BST 2021
Job ID          : 168314
Job name        : edgebce
WorkDir         : /mainfs/home/jlc1n20/projects/cluster_gnn
Command         : /mainfs/home/jlc1n20/projects/cluster_gnn/scripts/sub_bce.sh
Partition       : gpu
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : indigo58
Job Output Follows ...
===============================================================================
  0%|          | 0/90000 [00:00<?, ?it/s]  0%|          | 1/90000 [00:01<42:46:57,  1.71s/it]  0%|          | 2/90000 [00:01<21:01:45,  1.19it/s]  0%|          | 3/90000 [00:02<13:12:05,  1.89it/s]  0%|          | 4/90000 [00:02<10:08:34,  2.46it/s]  0%|          | 5/90000 [00:02<9:23:38,  2.66it/s]   0%|          | 6/90000 [00:02<7:17:05,  3.43it/s]  0%|          | 7/90000 [00:02<5:56:24,  4.21it/s]  0%|          | 8/90000 [00:03<6:01:39,  4.15it/s]  0%|          | 9/90000 [00:03<5:48:21,  4.31it/s]  0%|          | 10/90000 [00:03<4:48:19,  5.20it/s]  0%|          | 11/90000 [00:03<5:02:13,  4.96it/s]  0%|          | 12/90000 [00:03<4:47:14,  5.22it/s]  0%|          | 12/90000 [00:04<9:05:10,  2.75it/s]
Traceback (most recent call last):
  File "/home/jlc1n20/projects/cluster_gnn/scripts/train_bce.py", line 32, in <module>
    tot_loss += train(data.to(device))
  File "/home/jlc1n20/projects/cluster_gnn/scripts/train_bce.py", line 21, in train
    edge_pred = torch.sigmoid(model(data))
  File "/scratch/jlc1n20/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mainfs/home/jlc1n20/projects/cluster_gnn/src/cluster_gnn/models/gnn.py", line 74, in forward
    edge_attrs, node_attrs = self.conv8(node_attrs, data.edge_index, edge_attrs)
  File "/scratch/jlc1n20/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/mainfs/home/jlc1n20/projects/cluster_gnn/src/cluster_gnn/models/gnn.py", line 23, in forward
    return self.propagate(
  File "/scratch/jlc1n20/miniconda3/envs/PyG/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 237, in propagate
    out = self.message(**msg_kwargs)
  File "/mainfs/home/jlc1n20/projects/cluster_gnn/src/cluster_gnn/models/gnn.py", line 34, in message
    self.edge_embed = self.mlp_edge(recv_send)
  File "/scratch/jlc1n20/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/scratch/jlc1n20/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/scratch/jlc1n20/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/scratch/jlc1n20/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/scratch/jlc1n20/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 284.00 MiB (GPU 0; 15.78 GiB total capacity; 12.88 GiB already allocated; 25.50 MiB free; 14.51 GiB reserved in total by PyTorch)
==============================================================================
Running epilogue script on indigo58.

Submit time  : 2021-05-09T11:23:21
Start time   : 2021-05-09T11:23:21
End time     : 2021-05-09T11:24:22
Elapsed time : 00:01:01 (Timelimit=1-00:00:00)

Job ID: 168314
Cluster: i5
User/Group: jlc1n20/fp
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:13
CPU Efficiency: 21.31% of 00:01:01 core-walltime
Job Wall-clock time: 00:01:01
Memory Utilized: 3.43 GB
Memory Efficiency: 8.57% of 40.00 GB

